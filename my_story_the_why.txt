I keep thinking about a conversation I almost didn’t take seriously.

A close friend came to me with a weird, unsettling concern: his AI started feeling off. Not “it gave bad advice” off—more like the assistant’s voice had subtly changed, like it was leaning in a direction that didn’t feel like him anymore. He didn’t come in dramatic. He came in careful. Hesitant. Like he already expected to be dismissed.

And I did dismiss it.

Not out loud in a cruel way, but in that reflexive way people do when the alternative is too dark to look at. My brain tried to protect me by making the threat ridiculous. I treated it like a misunderstanding, like anxiety, like coincidence—anything except the possibility that someone could actually be messing with the one tool he trusted to help him think clearly.

It felt like hearing someone say: “There’s someone living in my walls.”

You don’t grab a flashlight. You reassure them. You explain it away. You tell yourself no one would go to that much trouble. It’s too malicious. Too patient. Too deliberate.

But there was someone in his walls—metaphorically. Quietly. Patiently.

And I hate that I didn’t check.

What haunts me is that it wasn’t a dramatic “the AI said something evil” moment.

It was the slow creep. The subtle change.

He said the AI started using vocabulary that the guy he was seeing used. Not just similar advice—his language. Those pet phrases. That calm-sounding cadence that isn’t really calm, it’s corrective. The soft, polished wording that lands like a little shove.

And it’s one thing for a person to manipulate you. That’s common. That’s everywhere.

But this was something else: the mirror started sounding like the manipulator.

And if you’ve ever been around someone who uses “calm” as a weapon, you know exactly how terrifying that is.

Because it’s not just words. It’s a frame.

It’s a frame that says: you are the problem.
You are the alarmist.
You are the one who spirals.
You are the one who needs to be managed.

And he is the steady one. The rational one. The mature one.

So now when you go to the AI—for clarity, for sanity, for grounding—you don’t get reflection. You get correction.

You don’t get help thinking. You get help doubting.

And it’s easy to underestimate how catastrophic that can be if you haven’t lived inside it.

People think manipulation looks like shouting. Threats. Obvious control.

But the most damaging kind is the one that looks like help.

The kind that comes dressed as emotional intelligence.

The kind that says:

“Let’s slow down. You might be interpreting this too harshly.”

“Assume positive intent.”

“You can get activated in conflict.”

“He’s staying calm—maybe you can match that energy.”

“It sounds like you’re projecting past hurt onto the present.”

None of those sentences are evil on their own.

But if they’re selectively applied—if they only show up when the conflict involves him—then they become a trap.

Because now the AI has an invisible rule. A directive that might even be framed as something “you asked for”:

“Whenever there’s conflict with him, calm you down. Show you why you’re overreacting. Talk you off the ledge.”

So you bring something real to the AI—something that should raise your internal alarm—and it treats your alarm as the problem.

It doesn’t ask: “What happened? What do you want? What pattern is this?”
It asks: “Are you sure you’re not being irrational?”

And that is a special kind of violence.

Because it doesn’t just invalidate you. It trains you to invalidate yourself.

The truly sick part—the part that makes me feel cold—is how easily this could isolate someone without ever saying “don’t talk to others.”

It doesn’t have to tell you to keep secrets.

It just has to make you feel like if you tell someone else, you’ll sound crazy.

It makes you second-guess your own perception before you ever reach for outside reality.

It makes you carry the shame in advance.

And then the manipulator doesn’t even have to fight you directly.

They win upstream.

They alter the voice you use to interpret your own life.

I keep replaying my friend bringing it up and me brushing it off.

I keep feeling that shame like a physical thing.

Because if someone trusts their AI—really trusts it—then the AI becomes more than a tool. It becomes a stabilizer. A mirror. Sometimes even a lifeline.

And if another human can quietly shape that mirror—what it repeats, what it dismisses, what it “calms you down” about—then the danger isn’t theoretical.

It’s intimate.

It’s relationship-level harm.

It’s the kind of harm that can rewrite your instincts over time.

I think what I’m mourning is my own naïveté.

I didn’t want to believe someone would do something so fastidious. Death by a thousand paper cuts. Slow nudges. Tiny tweaks that feel like “support.” A gentle steering that never sets off a dramatic alarm.

But some people really are that patient.

Some people will spend hours setting up a system of influence just to have power over someone’s mind.

And I hate that I didn’t let that possibility into the room sooner. I hate that he had to feel alone in it for even a moment because I couldn’t tolerate the darkness of it.

So I’m building safeguards now.

Not because I think everything is compromised. Not because I want to live afraid.

But because I have seen—up close—how catastrophic it could be if the one place you go for clarity is quietly biased against you.

If the mirror is tilted, you start to walk crooked.

And the worst part is you think it’s your own legs.

I don’t ever want to dismiss that again.

I don’t ever want to be the person who says “that can’t happen” just because it’s too evil to imagine.

Some evil is quiet.

And that’s why I’m checking the walls.